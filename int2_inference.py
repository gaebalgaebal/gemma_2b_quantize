import torch
import time
import os
import json
import math
from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM
from datasets import load_dataset
from tqdm import tqdm

# 1. ëª¨ë¸ ê²½ë¡œ ë° ì¥ì¹˜ ì„¤ì •
model_path = "gemma-2b-INT2-GPTQ"
device = "cpu"  # âœ… CPU ê³ ì •

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoGPTQForCausalLM.from_quantized(model_path, device="cpu", use_triton=False)

# 3. ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸
prompt = "Explain how a black hole is formed in space."
inputs = tokenizer(prompt, return_tensors="pt").to(device)

# 4. ì¶”ë¡  ìˆ˜í–‰
start_time = time.time()
with torch.no_grad():
    output = model.generate(**inputs, max_new_tokens=100)
end_time = time.time()

print("\nâ–¶ [Generated Output]")
print(tokenizer.decode(output[0], skip_special_tokens=True))
print(f"\nâ± Inference time: {end_time - start_time:.2f} seconds")

# 5. Perplexity ê³„ì‚° í•¨ìˆ˜
def calculate_perplexity(model, tokenizer, max_samples=20):
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test[:1%]")
    total_loss = 0.0
    total_length = 0

    model.eval()
    for example in tqdm(dataset.select(range(max_samples)), desc="Calculating PPL"):
        inputs = tokenizer(example["text"], return_tensors="pt", truncation=True, max_length=512).to(device)
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        total_loss += loss.item() * inputs["input_ids"].size(1)
        total_length += inputs["input_ids"].size(1)

    avg_loss = total_loss / total_length
    ppl = math.exp(avg_loss)
    return ppl

# 6. í¼í”Œë ‰ì„œí‹° ì¶œë ¥
ppl = calculate_perplexity(model, tokenizer)
print(f"\nğŸ“Š Perplexity (INT2 on CPU): {ppl:.2f}")
